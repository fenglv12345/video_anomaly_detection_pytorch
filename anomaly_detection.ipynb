{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from convolution_lstm import ConvLSTMCell, ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import _pair\n",
    "\n",
    "class ConvLSTM2DCell(nn.Module):\n",
    "  \"\"\"KERAS Cell class for the ConvLSTM2D layer.\n",
    "  Arguments:\n",
    "    filters: Integer, the dimensionality of the output space\n",
    "      (i.e. the number of output filters in the convolution).\n",
    "    kernel_size: An integer or tuple/list of n integers, specifying the\n",
    "      dimensions of the convolution window.\n",
    "    strides: An integer or tuple/list of n integers,\n",
    "      specifying the strides of the convolution.\n",
    "      Specifying any stride value != 1 is incompatible with specifying\n",
    "      any `dilation_rate` value != 1.\n",
    "    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n",
    "    data_format: A string,\n",
    "      one of `channels_last` (default) or `channels_first`.\n",
    "      It defaults to the `image_data_format` value found in your\n",
    "      Keras config file at `~/.keras/keras.json`.\n",
    "      If you never set it, then it will be \"channels_last\".\n",
    "    dilation_rate: An integer or tuple/list of n integers, specifying\n",
    "      the dilation rate to use for dilated convolution.\n",
    "      Currently, specifying any `dilation_rate` value != 1 is\n",
    "      incompatible with specifying any `strides` value != 1.\n",
    "    activation: Activation function to use.\n",
    "      If you don't specify anything, no activation is applied\n",
    "      (ie. \"linear\" activation: `a(x) = x`).\n",
    "    recurrent_activation: Activation function to use\n",
    "      for the recurrent step.\n",
    "    use_bias: Boolean, whether the layer uses a bias vector.\n",
    "    kernel_initializer: Initializer for the `kernel` weights matrix,\n",
    "      used for the linear transformation of the inputs.\n",
    "    recurrent_initializer: Initializer for the `recurrent_kernel`\n",
    "      weights matrix,\n",
    "      used for the linear transformation of the recurrent state.\n",
    "    bias_initializer: Initializer for the bias vector.\n",
    "    unit_forget_bias: Boolean.\n",
    "      If True, add 1 to the bias of the forget gate at initialization.\n",
    "      Use in combination with `bias_initializer=\"zeros\"`.\n",
    "      This is recommended in [Jozefowicz et al.]\n",
    "      (http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n",
    "    kernel_regularizer: Regularizer function applied to\n",
    "      the `kernel` weights matrix.\n",
    "    recurrent_regularizer: Regularizer function applied to\n",
    "      the `recurrent_kernel` weights matrix.\n",
    "    bias_regularizer: Regularizer function applied to the bias vector.\n",
    "    kernel_constraint: Constraint function applied to\n",
    "      the `kernel` weights matrix.\n",
    "    recurrent_constraint: Constraint function applied to\n",
    "      the `recurrent_kernel` weights matrix.\n",
    "    bias_constraint: Constraint function applied to the bias vector.\n",
    "    dropout: Float between 0 and 1.\n",
    "      Fraction of the units to drop for\n",
    "      the linear transformation of the inputs.\n",
    "    recurrent_dropout: Float between 0 and 1.\n",
    "      Fraction of the units to drop for\n",
    "      the linear transformation of the recurrent state.\n",
    "  Call arguments:\n",
    "    inputs: A 4D tensor.\n",
    "    states:  List of state tensors corresponding to the previous timestep.\n",
    "    training: Python boolean indicating whether the layer should behave in\n",
    "      training mode or in inference mode. Only relevant when `dropout` or\n",
    "      `recurrent_dropout` is used.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               in_channels,\n",
    "               out_channels,\n",
    "               kernel_size,\n",
    "               stride=(1, 1),\n",
    "               padding=0,\n",
    "               padding_mode='zeros',\n",
    "               #data_format=None,\n",
    "               dilation_rate=(1, 1),\n",
    "               #activation='tanh',\n",
    "               #recurrent_activation='hard_sigmoid',\n",
    "               #use_bias=True,\n",
    "               #kernel_initializer='glorot_uniform',\n",
    "               #recurrent_initializer='orthogonal',\n",
    "               #bias_initializer='zeros',\n",
    "               #unit_forget_bias=True,\n",
    "               #kernel_regularizer=None,\n",
    "               #recurrent_regularizer=None,\n",
    "               #bias_regularizer=None,\n",
    "               #kernel_constraint=None,\n",
    "               #recurrent_constraint=None,\n",
    "               #bias_constraint=None,\n",
    "               dropout=0.,\n",
    "               recurrent_dropout=0.,\n",
    "               **kwargs):\n",
    "    super(ConvLSTM2DCell, self).__init__(**kwargs)\n",
    "    self.filters = filters\n",
    "    self.kernel_size = _pair(kernel_size)\n",
    "    self.strides = _pair(strides)\n",
    "    self.padding = conv_utils.normalize_padding(padding)\n",
    "    #self.data_format = conv_utils.normalize_data_format(data_format)\n",
    "    self.dilation_rate = _pair(dilation_rate)\n",
    "    #self.activation = activations.get(activation)\n",
    "    #self.recurrent_activation = activations.get(recurrent_activation)\n",
    "    #self.use_bias = use_bias\n",
    "\n",
    "    #self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "    #self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "    #self.bias_initializer = initializers.get(bias_initializer)\n",
    "    #self.unit_forget_bias = unit_forget_bias\n",
    "\n",
    "    #self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "    #self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
    "    #self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "    #self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "    #self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
    "    #self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "    #self.dropout = min(1., max(0., dropout))\n",
    "    #self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
    "    #self.state_size = (self.filters, self.filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/layers/convolutional_recurrent.py\n",
    "https://arxiv.org/pdf/1506.04214v2.pdf\n",
    "https://arxiv.org/pdf/1701.01546.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoAutoencoderLSTM(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.conv_encoder = nn.Sequential(OrderedDict([\n",
    "              ('conv1', nn.Conv3d(in_channels=1, out_channels=128, kernel_size=(11,11,1),stride=(4,4,1), padding=0)),\n",
    "              ('nonl1', nn.Tanh()),\n",
    "              ('conv2', nn.Conv3d(in_channels=128, out_channels=64, kernel_size=(5,5,1),stride=(2,2,1), padding=0)),\n",
    "              ('nonl2', nn.Tanh())\n",
    "            ]))\n",
    "        self.rnn_encoder = ConvLSTMCell()\n",
    "        self.rnn_bottleneck = ConvLSTMCell()\n",
    "        self.rnn_decoder = ConvLSTMCell()\n",
    "        self.conv_decoder = nn.Sequential(OrderedDict([\n",
    "              ('deconv1', nn.ConvTranspose3d(1,20,5)),\n",
    "              ('nonl1', nn.Tanh()),\n",
    "              ('deconv2', nn.ConvTranspose3d(20,64,5)),\n",
    "              ('nonl2', nn.Tanh())\n",
    "            ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        model.add(Conv3D(filters=128,kernel_size=(11,11,1),strides=(4,4,1),padding='valid',input_shape=(227,227,10,1),activation='tanh'))\n",
    "\tmodel.add(Conv3D(filters=64,kernel_size=(5,5,1),strides=(2,2,1),padding='valid',activation='tanh'))\n",
    "\n",
    "\n",
    "\n",
    "\tmodel.add(ConvLSTM2D(filters=64,kernel_size=(3,3),strides=1,padding='same',dropout=0.4,recurrent_dropout=0.3,return_sequences=True))\n",
    "\n",
    "\t\n",
    "\tmodel.add(ConvLSTM2D(filters=32,kernel_size=(3,3),strides=1,padding='same',dropout=0.3,return_sequences=True))\n",
    "\n",
    "\n",
    "\tmodel.add(ConvLSTM2D(filters=64,kernel_size=(3,3),strides=1,return_sequences=True, padding='same',dropout=0.5))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\tmodel.add(Conv3DTranspose(filters=128,kernel_size=(5,5,1),strides=(2,2,1),padding='valid',activation='tanh'))\n",
    "\tmodel.add(Conv3DTranspose(filters=1,kernel_size=(11,11,1),strides=(4,4,1),padding='valid',activation='tanh'))\n",
    "\n",
    "\tmodel.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # gradient check\n",
    "    convlstm = ConvLSTM(input_channels=512, hidden_channels=[128, 64, 64, 32, 32], kernel_size=3, step=5,\n",
    "                        effective_step=[4]).cuda()\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    input = Variable(torch.randn(1, 512, 64, 32)).cuda()\n",
    "    target = Variable(torch.randn(1, 32, 64, 32)).double().cuda()\n",
    "\n",
    "    output = convlstm(input)\n",
    "    output = output[0][0].double()\n",
    "    res = torch.autograd.gradcheck(loss_fn, (output, target), eps=1e-6, raise_exception=True)\n",
    "    print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
